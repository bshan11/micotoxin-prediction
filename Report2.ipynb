{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcec062-97cd-42c0-a4bd-a64d16595521",
   "metadata": {},
   "source": [
    "# **Report 2: Mycotoxin Prediction in Corn Using Machine Learning**\n",
    "\n",
    "## - **Clipping Method is used for handling outlliers in target variable**\r\n",
    "\r\n",
    "## **1. Preprocessing Steps & Rationale**\r\n",
    "\r\n",
    "### **1.1 Data Cleaning & Normalization**\r\n",
    "- **Outlier Removal:** Applied the **Interquartile Range (IQR) method** to remove extreme values, ensuring model stability.\r\n",
    "- **Feature Scaling:** Used **MinMaxScaler** to normalize spectral reflectance data, ensuring all features are within the same range for better model convergence.\r\n",
    "- **Clipping Extreme Values:** Instead of log transformation, we applied **IQR-based value clipping** to the target variable.\r\n",
    "  - **Rationale:** This prevents extreme target values from distorting predictions while maintaining meaningful van levels.\r\n",
    "\r\n",
    "### **1.2 Dimensionality Reduction (PCA)**\r\n",
    "- **Applied Principal Component Analysis (PCA)** to retain **95% variance**, reducing 448 spectral bands to the top 5 most important components.\r\n",
    "- **Rationale:** PCA helped remove noise and redundant information, improving model interpretability.\r\n",
    "- **Insights:** **PC2 and PC3 were consistently the most important features across all models.**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **2. Model Selection, Training & Evaluation**\r\n",
    "\r\n",
    "### **2.1 Models Tested**\r\n",
    "| Model | MAE (Lower is Better) | RMSE (Lower is Better) | R² Score (Higher is Better) |\r\n",
    "|--------|----------------|----------------|--------------|\r\n",
    "| **Random Forest** | ✅ 664.61 | ✅ 892.87 | 🎯 **0.5275** ✅ |\r\n",
    "| **XGBoost** | ✅ 650.61 | ✅ 934.76 | ✅ 0.4822 |\r\n",
    "| **Tuned XGBoost** | ✅ 695.77 | ✅ 921.78 | ✅ 0.4964 |\r\n",
    "| **MLP (500 iterations)** | ❌ 768.95 | ❌ 1059.00 | ❌ 0.335 |\r\n",
    "| **MLP (2500 iterations)** | ✅ 702.12 | ✅ 943.41 | ✅ 0.4725 |\r\n",
    "| **MLP (Tuned)** | ✅ 698.26 | ✅ 1020.17 | ✅ 0.3832 |\r\n",
    "\r\n",
    "### **2.2 Best Model: Random Forest**\r\n",
    "- **Performance:**\r\n",
    "  - **MAE:** **664.61**\r\n",
    "  - **RMSE:** **892.87**\r\n",
    "  - **R² Score:** **0.5275** (Best among all models tested)\r\n",
    "- **Feature Importance Insights:**\r\n",
    "  - **PC2 (43.6%) & PC3 (23.9%) were the most important features**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **3. Key Findings & Suggestions for Improvement**\r\n",
    "\r\n",
    "### **3.1 Key Findings**\r\n",
    "✅ **Random Forest performed best** among all models tested based on R² score.  \r\n",
    "✅ **XGBoost showed competitive results**, but tuning did not significantly improve performance.  \r\n",
    "✅ **MLP required more iterations to improve performance but still lagged behind tree-based models.**  \r\n",
    "✅ **PCA significantly improved model accuracy** by removing redundant spectral bands.  \r\n",
    "✅ **IQR-based value clipping effectively handled extreme values**, but further analysis is needed to confirm its effectiveness compared to log transformation.\r\n",
    "\r\n",
    "### **3.2 Observations**\r\n",
    "- **Random Forest performed best overall, suggesting that simpler models work well for this dataset.**\r\n",
    "- **MLP models required significantly more iterations to approach tree-based models' performance.**\r\n",
    "- **XGBoost tuning did not lead to meaningful improvements, possibly due to data characteristics.**\r\n",
    "- **PC2 and PC3 were consistently the most important principal components.**\r\n",
    "\r\n",
    "### **3.3 Areas for Improvement**\r\n",
    "🚀 **Explore Hybrid Models** → Combine RF + XGB for ensemble learning.  \r\n",
    "🚀 **Try Log Transformation Again** → Compare its effectiveness against clipping for handling extreme values.  \r\n",
    "🚀 **Increase Dataset Size** → Deep learning models (MLP) may improve with more training data.  \r\n",
    "🚀 **Optimize Feature Engineering** → Instead of PCA, end alternative feature engineering approaches should be explored.**\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
